{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d5b4f43",
   "metadata": {},
   "source": [
    "<center style=\"font-size: 30px\">\n",
    "\n",
    "# DAA Assignment\n",
    "\n",
    "</center>\n",
    "\n",
    "- [Team Members](#team-members)\n",
    "- [Introduction](#introduction)\n",
    "- [Problem Statement](#problem-statement)\n",
    "- [Algorithm](#algorithm)\n",
    "  - [Ford Fulkerson](#Ford-Fulkerson)\n",
    "  - [Segmented Least Squares](#Segmented-Least-Squares)\n",
    "- [Helper Functions](#helper-function)\n",
    "- [Analysis and Conclusions](#analysis-and-conclusions)\n",
    "\n",
    "<a name=\"team-members\"></a>\n",
    "\n",
    "## Team Members\n",
    "\n",
    "<style>\n",
    "td, th {\n",
    "   border: none!important;\n",
    "   font-size: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "| NAME                        | ID            |\n",
    "| --------------------------- | ------------- |\n",
    "| Milind Jain                 | 2020A7PS0153H |\n",
    "| Mokshith Naidu Thakkilapati | 2020A7PS1885H |\n",
    "| Anish Kumar Kallepalli      | 2020A7PS0282H |\n",
    "| Sriram Srivatsan            | 2020A7PS0273H |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3ac76c4",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The maximum flow problem involves finding the maximum amount of flow that can be sent from a source node to a sink node in a directed graph, subject to capacity constraints on the edges. The Ford-Fulkerson algorithm is an algorithm for solving the maximum flow problem in a flow network.\n",
    "\n",
    "The minimum s-t cut problem is closely related to the maximum flow problem, in fact, it can be shown that the maximum flow is equal to the minimum capacity of all s-t cuts. This is known as the max-flow min-cut theorem.\n",
    "\n",
    "A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that all edges connect a vertex in one set to a vertex in the other set. The Bipartite Matching problem is a graph optimization problem that involves finding the largest possible matching between two disjoint sets of vertices in a bipartite graph. There can be more than one maximum matchings for a given Bipartite Graph. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4dd5bcb",
   "metadata": {},
   "source": [
    "<a name=\"problem-statement\"></a>\n",
    "\n",
    "## Problem Statements\n",
    "\n",
    "###  Problem Statement 1\n",
    "We are supposed to implement the Ford-Fulkerson Algorithm that was explained in class. Then we should implement the subroutine to find the minimum st-cut of a network flow graph. We are also supposed to use the Ford-Fulkerson algorithm for solving Bipartite Matching problem. The Bipartite Matching problem can be stated as follows: Given a bipartite graph G = (U, V, E), where U and V are the disjoint sets of vertices and E is the set of edges connecting vertices in U to vertices in V, the goal is to find a maximum cardinality matching M, which is a subset of E such that no two edges in M share a common endpoint. We are also supposed to run the algorithm on different kinds of network flow graphs, such as smaller graphs to test the code and larger graphs to verify the robustness of implementations.\n",
    "\n",
    "###  Problem Statement 2\n",
    "Assume a set P of n points in the plane, labelled (x1,y1), (x2,y2), (x3,y3),..., (xn,yn) and suppose x1 < x2 < …< xn. Divide P into a few parts. Segment is a subset of P that represents a contiguous set of x-coordinates. Compute the line minimizing the error with respect to the points in S. Along with the error, we want to penalise having too many partitions.Penalty is calculated as the sum of the segments into which P is divided times a constant multiplier C > 0 and for each segment, the error value of the optimal line through that segment. Our aim is to find a partition with the least amount of penalty."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2e6eb2f",
   "metadata": {},
   "source": [
    "<a name=\"algorithm\"></a>\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "<a name=\"Ford-Fulkerson\"></a>\n",
    "\n",
    "### Ford-Fulkerson\n",
    "\n",
    "The algorithm finds augmenting paths repeatedly in the residual graph, which is a graph that represents the capacity of the remaining flow after the initial flow has been subtracted, starting with an initial flow of zero. In the residual graph, an augmenting path is one that has positive capacity on all of its edges and connects the source and sink nodes. The algorithm increases the flow along an augmenting path by the minimum capacity of the edges along the path once it has been identified, effectively pushing more flow from the source node to the sink node. The flow is at its highest point when this process is repeated until no more augmenting paths can be discovered. There are several ways to find augmenting paths when using the Ford-Fulkerson algorithm, including breadth-first search and depth-first search. If the capacities are not integral, one important factor to take into account is that the algorithm might not always converge to the maximum flow. Hence we use a updated Ford-Fulkerson algorithm that uses the shortest augmenting path instead of any augmenting path. This ensures that the algorithm will terminate in a finite number of iterations, since the length of the shortest augmenting path can only decrease after each iteration.\n",
    "\n",
    "The algorithm has a worst-case time complexity of $O(m^2log C)$ , where m is the number of edges in the flow network and c is the maximum flow. However, in practice, the algorithm tends to perform much better than this worst-case bound.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7882d50",
   "metadata": {},
   "source": [
    "<a name=\"Segmented-Least-Squares\"></a>\n",
    "\n",
    "### Segmented Least Squares\n",
    "\n",
    "The segmented least squares problem can be solved with dynamic programming.\n",
    "First we calculate errors between each pair of points e(i,j) using the formula :\n",
    "\n",
    "$$ e_{ij} : \\sum_{i}^{n} {(y_i \\, – \\, ax_i \\, – \\, b^2)}^2 $$\n",
    "\n",
    "$$ a :   \\frac{\\sum_{i}^{n} {x_i\\,y_i} - (\\sum_{i} {x_i})(\\sum_{i=1} {y_i})}{\\sum_{i}^{n} {x_i^2}-(\\sum_{i} {x_i})^2}$$\n",
    " \n",
    "$$ b :   \\frac{\\sum_{i} {y_i} - a\\sum_{i} {x_i}}{n} $$\n",
    " \n",
    "Let $OPT_j$ denote the minimum cost for points $p_1, p_2,\\dots, p_j$. Let $e_{ij}$ denote the minimum squared error for points $p_i, p_{i+1},\\dots, p_j$. The crucial observation is that the last point $p_j$ for some subproblem $OPT_j$ belongs to a single segment in the optimal partition, and that segment begins at some earlier point pi. Thus, if we knew $OPT_{i-1}$, we could compute $OPT_j$. This leads to the following recursive formulation:\n",
    "$$ OPT_j = min_{1 \\le i \\le j}(e_{ij} + C + OPT_{i-1}) $$\n",
    "Here $C$ is the penalty for each segement is taken as input <br>\n",
    "$OPT_n$ gives us the minimum penalty for all the $n$ points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c0be33",
   "metadata": {},
   "source": [
    "<a name=\"helper-function\"></a>\n",
    "\n",
    "## Helper Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5f509d",
   "metadata": {},
   "source": [
    "```\n",
    "vector<int> get_path(int start, int end, vector<vector<int>> &res_adj, int del)\n",
    "```\n",
    "\n",
    "Function to get a path with a source vertex, sink vertex using BFS such that every edge on the path has a weight greater than delta.\n",
    "<br>Time complexity is $O(N+M')$, where $N$ is number of vertices and $M'$ is number of edges in the residual graph.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d83392e5",
   "metadata": {},
   "source": [
    "```\n",
    "void augment(vector<int> &f, vector<int> &path, vector<vector<int>> &res_adj, map<array<int, 2>, int> &edg_to_i)\n",
    "```\n",
    "\n",
    "Function to update the adjacency matrix of the residual graph with the bottleneck of the path obtained. Such that the forward edge weight will be reduced by the bottleneck and the backward edge weight will be increased by the bottleneck.\n",
    "<br>Time complexity is $O(|P|)$, where $P$ is the path and $|P|$ is the length of the path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "449c6d9d",
   "metadata": {},
   "source": [
    "```\n",
    "void reach_from_source(int u, vector<bool> &vis, vector<vector<int>> &res_adj)\n",
    "```\n",
    "\n",
    "Function to get all the vertices that can be visited from source vertex. This is done by iterating through all the vertices that are not visited and if there is an edge between u and this vertex, recursively call this function to find all the points u can reach from this point.\n",
    "<br>Time complexity is $O(N+M')$, where $N$ is number of vertices and $M'$ is number of edges in the residual graph.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "217c7eaa",
   "metadata": {},
   "source": [
    "<a name=\"analysis-and-conclusions\"></a>\n",
    "\n",
    "## Analysis and Conclusions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8697de1a",
   "metadata": {},
   "source": [
    "We have decomposed the polygons using the above mentioned algorithms such as the decomposition and merging to generate the polygons of the partitions We ran our code on many datasets of polygons including a list of all the countries represented as a polygon with a set of vertices.For the sake of brevity, we do not present the results individually for every the polygon.\n",
    "\n",
    "We have used a visualizer program which allows use to view the polygon after it is decomposed to convex polygons. This are some of the results we have gathered by running our code on the polygon diagram of some countries, such as Japan, Ukraine, India, USA, Canada, Russia which have 37, 98, 136, 233, 272, 447 vertices respectively.\n",
    "\n",
    "All the codes were implemented on g++ (MinGW.org GCC Build-2) 9.2.0 and run on a PC with a processor 12th Gen Intel(R) Core(TM) i7-12700H with a 2.30 GHz speed.\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16fd5310",
   "metadata": {},
   "source": [
    "<a name=\"acp1\"></a>\n",
    "\n",
    "### Problem Statement 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dee95eb3",
   "metadata": {},
   "source": [
    "#### Russia: <br>\n",
    "\n",
    "<img src=\"images/Russia.png\" align=\"right\" height=\"150\" width=\"300\"/>\n",
    "Number of notches: 215 <br>\n",
    "Number of vertices: 447 <br>\n",
    "Number of faces after decomposition: 256 <br>\n",
    "Number of faces after merging: 186 <br>\n",
    "Avg Time for decomposing is: 34.466 ms <br>\n",
    "Avg Time for merging is: 6.4668 ms\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61a09304",
   "metadata": {},
   "source": [
    "| Polygon | Vertices | Notches | Polygons | Avg Dec. time | Std Dec. time. | Merg. poly. | Avg Merg. time | Std Merg. time. |\n",
    "| ------- | -------- | ------- | -------- | ------------- | -------------- | ----------- | -------------- | --------------- |\n",
    "| Japan   | 37       | 15      | 19       | 1.2466 ms     | 0.535 ms       | 16          | 0.2 ms         | 0.447 ms        |\n",
    "| Ukraine | 98       | 46      | 54       | 2.733 ms      | 0.816 ms       | 41          | 0.41 ms        | 0.562 ms        |\n",
    "| India   | 136      | 66      | 75       | 3.9044 ms     | 0.419 ms       | 53          | 0.7936 ms      | 0.444 ms        |\n",
    "| USA     | 233      | 111     | 112      | 13.549 ms     | 0.513 ms       | 82          | 2.055 ms       | 0.253 ms        |\n",
    "| Canada  | 272      | 130     | 144      | 14.476 ms     | 0.639 ms       | 107         | 2.1638 ms      | 0.266 ms        |\n",
    "| Russia  | 447      | 215     | 256      | 34.466 ms     | 1.189 ms       | 186         | 6.4668 ms      | 0.533 ms        |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23cf3139",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can conclude from our observations that as the number of vertices increases the time taken to decompose and merge the polygons increases. From the images we can conclude that the polygons are all convex. Hence we can conclude the implementation of the algorithm was correct.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff5950b",
   "metadata": {},
   "source": [
    "<a name=\"acp1\"></a>\n",
    "\n",
    "### Problem Statement 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44f2e853",
   "metadata": {},
   "source": [
    "#### Russia: <br>\n",
    "\n",
    "<img src=\"images/Russia.png\" align=\"right\" height=\"150\" width=\"300\"/>\n",
    "Number of notches: 215 <br>\n",
    "Number of vertices: 447 <br>\n",
    "Number of faces after decomposition: 256 <br>\n",
    "Number of faces after merging: 186 <br>\n",
    "Avg Time for decomposing is: 34.466 ms <br>\n",
    "Avg Time for merging is: 6.4668 ms\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0e2c0a",
   "metadata": {},
   "source": [
    "| Polygon | Vertices | Notches | Polygons | Avg Dec. time | Std Dec. time. | Merg. poly. | Avg Merg. time | Std Merg. time. |\n",
    "| ------- | -------- | ------- | -------- | ------------- | -------------- | ----------- | -------------- | --------------- |\n",
    "| Japan   | 37       | 15      | 19       | 1.2466 ms     | 0.535 ms       | 16          | 0.2 ms         | 0.447 ms        |\n",
    "| Ukraine | 98       | 46      | 54       | 2.733 ms      | 0.816 ms       | 41          | 0.41 ms        | 0.562 ms        |\n",
    "| India   | 136      | 66      | 75       | 3.9044 ms     | 0.419 ms       | 53          | 0.7936 ms      | 0.444 ms        |\n",
    "| USA     | 233      | 111     | 112      | 13.549 ms     | 0.513 ms       | 82          | 2.055 ms       | 0.253 ms        |\n",
    "| Canada  | 272      | 130     | 144      | 14.476 ms     | 0.639 ms       | 107         | 2.1638 ms      | 0.266 ms        |\n",
    "| Russia  | 447      | 215     | 256      | 34.466 ms     | 1.189 ms       | 186         | 6.4668 ms      | 0.533 ms        |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a53614b1",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can conclude from our observations that as the number of vertices increases the time taken to decompose and merge the polygons increases. From the images we can conclude that the polygons are all convex. Hence we can conclude the implementation of the algorithm was correct.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
